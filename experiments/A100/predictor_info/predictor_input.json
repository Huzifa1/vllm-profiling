{
    "labels": [
        "falcon-7b",
        "llama2-13b-hf",
        "llama2-7b-hf",
        "llama3-3b",
        "qwen-0.5b",
        "qwen-1.8b",
        "qwen-14b",
        "qwen-4b",
        "qwen-7b",
        "yi-6b",
        "yi-9b"
    ],
    "data": {
        "load_weights": [
            8.14,
            14.35,
            7.578,
            3.618,
            0.584,
            2.178,
            15.478,
            4.524,
            8.736,
            6.756,
            9.713999999999999
        ],
        "model_init": [
            0.20400000000000001,
            0.22000000000000003,
            0.20800000000000002,
            0.244,
            0.18,
            0.192,
            0.22600000000000003,
            0.22200000000000003,
            0.20800000000000002,
            0.20800000000000002,
            0.23199999999999998
        ],
        "dynamo_transform_time": [
            7.2440000000000015,
            9.988,
            8.296,
            7.44,
            6.494,
            6.736,
            10.184,
            10.309999999999999,
            8.506,
            8.303999999999998,
            11.652000000000001
        ],
        "graph_compile_general_shape": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "graph_compile_cached": [
            6.906000000000001,
            9.521,
            7.521000000000001,
            6.7104,
            5.3926,
            5.9184,
            9.8712,
            9.4036,
            7.6659999999999995,
            7.1049999999999995,
            10.003400000000001
        ],
        "kv_cache_profiling": [
            3.2680000000000007,
            4.945,
            2.262999999999998,
            2.639599999999998,
            2.019399999999999,
            1.8196000000000012,
            4.960799999999999,
            3.5063999999999993,
            2.233999999999998,
            2.1709999999999994,
            2.8766000000000034
        ],
        "graph_capturing": [
            1.374,
            1.9799999999999998,
            1.388,
            1.102,
            0.938,
            0.97,
            1.9920000000000002,
            1.394,
            1.374,
            1.346,
            1.8699999999999999
        ],
        "tokenizer_init": [
            0.182,
            0.134,
            0.132,
            0.454,
            0.39,
            0.38999999999999996,
            0.384,
            0.386,
            0.38400000000000006,
            0.246,
            0.25
        ],
        "total_time": [
            27.318,
            41.138000000000005,
            27.386,
            22.208,
            15.998,
            18.204,
            43.096000000000004,
            29.746,
            29.107999999999997,
            26.136,
            36.598
        ]
    },
    "models_config": {
        "qwen-0.5b": {
            "size": 0.5,
            "layers": 24,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 1024,
            "ffn_dim": 2816,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 19
        },
        "qwen-1.8b": {
            "size": 1.8,
            "layers": 24,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 2048,
            "ffn_dim": 5504,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 20
        },
        "qwen-4b": {
            "size": 4,
            "layers": 40,
            "num_key_value_heads": 20,
            "num_attention_heads": 20,
            "hidden_size": 2560,
            "ffn_dim": 6912,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 20
        },
        "qwen-7b": {
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 32,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 33
        },
        "qwen-14b": {
            "size": 14,
            "layers": 40,
            "num_key_value_heads": 40,
            "num_attention_heads": 40,
            "hidden_size": 5120,
            "ffn_dim": 13696,
            "vocab_size": 152064,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 33
        },
        "llama2-7b-hf": {
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 32,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_size": 20
        },
        "llama2-13b-hf": {
            "size": 13,
            "layers": 40,
            "num_key_value_heads": 40,
            "num_attention_heads": 40,
            "hidden_size": 5120,
            "ffn_dim": 13824,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_size": 20
        },
        "llama3-3b": {
            "size": 3,
            "layers": 28,
            "num_key_value_heads": 8,
            "num_attention_heads": 24,
            "hidden_size": 3072,
            "ffn_dim": 8192,
            "vocab_size": 128256,
            "tokenizer_size": 8.7,
            "compiled_graph_size": 24
        },
        "falcon-7b": {
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 1,
            "num_attention_heads": 71,
            "hidden_size": 4544,
            "ffn_dim": 18176,
            "vocab_size": 65024,
            "tokenizer_size": 2.7,
            "compiled_graph_size": 24
        },
        "yi-6b": {
            "size": 6,
            "layers": 32,
            "num_key_value_heads": 4,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5,
            "compiled_graph_size": 36
        },
        "yi-9b": {
            "size": 9,
            "layers": 48,
            "num_key_value_heads": 4,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5,
            "compiled_graph_size": 24
        }
    }
}