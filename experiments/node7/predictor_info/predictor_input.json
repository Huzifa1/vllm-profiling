{
    "labels": [
        "falcon-7b",
        "llama2-13b-hf",
        "llama2-7b-hf",
        "llama3-3b",
        "qwen-0.5b",
        "qwen-14b",
        "qwen-1.8b",
        "qwen-4b",
        "qwen-7b",
        "yi-6b",
        "yi-9b"
    ],
    "data": {
        "load_weights": [
            1.918,
            3.554,
            1.796,
            0.86,
            0.14200000000000002,
            3.7960000000000003,
            0.51,
            1.094,
            2.084,
            1.624,
            2.4299999999999997
        ],
        "model_init": [
            0.12,
            0.128,
            0.12,
            0.13,
            0.1,
            0.13,
            0.10600000000000001,
            0.122,
            0.12,
            0.118,
            0.136
        ],
        "dynamo_transform_time": [
            3.9799999999999995,
            5.9719999999999995,
            4.864,
            4.246,
            3.722,
            6.076,
            3.6980000000000004,
            6.146,
            4.968,
            4.994,
            6.996
        ],
        "graph_compile_general_shape": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "graph_compile_cached": [
            3.5338000000000003,
            4.639799999999999,
            3.6159999999999997,
            3.2998,
            2.6130000000000004,
            4.6936,
            2.7318,
            4.553400000000001,
            3.6760000000000006,
            3.7668,
            5.655799999999999
        ],
        "kv_cache_profiling": [
            2.1342,
            2.3202,
            1.3819999999999997,
            1.4582000000000015,
            1.367,
            2.3324,
            1.3762000000000008,
            1.5505999999999993,
            1.4019999999999992,
            1.3712,
            2.0142000000000007
        ],
        "graph_capturing": [
            1.118,
            1.434,
            1.0859999999999999,
            0.9960000000000001,
            0.908,
            1.486,
            0.9019999999999999,
            1.278,
            1.114,
            1.1060000000000003,
            1.452
        ],
        "tokenizer_init": [
            0.122,
            0.082,
            0.08,
            0.29,
            0.25,
            0.25,
            0.25,
            0.25,
            0.25,
            0.16,
            0.158
        ],
        "total_time": [
            12.926,
            18.13,
            12.944,
            11.280000000000001,
            9.102000000000002,
            18.764000000000003,
            9.574,
            14.994,
            13.614,
            13.14,
            18.842000000000002
        ]
    },
    "models_config": {
        "qwen-0.5b": {
            "size": 0.5,
            "layers": 24,
            "attn_heads": 16,
            "attn_heads_num": 16,
            "hidden_size": 1024,
            "ffn_dim": 2816,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 19
        },
        "qwen-1.8b": {
            "size": 1.8,
            "layers": 24,
            "attn_heads": 16,
            "attn_heads_num": 16,
            "hidden_size": 2048,
            "ffn_dim": 5504,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 20
        },
        "qwen-4b": {
            "size": 4,
            "layers": 40,
            "attn_heads": 20,
            "attn_heads_num": 20,
            "hidden_size": 2560,
            "ffn_dim": 6912,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 20
        },
        "qwen-7b": {
            "size": 7,
            "layers": 32,
            "attn_heads": 32,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 33
        },
        "qwen-14b": {
            "size": 14,
            "layers": 40,
            "attn_heads": 40,
            "attn_heads_num": 40,
            "hidden_size": 5120,
            "ffn_dim": 13696,
            "vocab_size": 152064,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 33
        },
        "llama2-7b-hf": {
            "size": 7,
            "layers": 32,
            "attn_heads": 32,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_size": 20
        },
        "llama2-13b-hf": {
            "size": 13,
            "layers": 40,
            "attn_heads": 40,
            "attn_heads_num": 40,
            "hidden_size": 5120,
            "ffn_dim": 13824,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_size": 20
        },
        "llama3-3b": {
            "size": 3,
            "layers": 28,
            "attn_heads": 8,
            "attn_heads_num": 24,
            "hidden_size": 3072,
            "ffn_dim": 8192,
            "vocab_size": 128256,
            "tokenizer_size": 8.7,
            "compiled_graph_size": 24
        },
        "falcon-7b": {
            "size": 7,
            "layers": 32,
            "attn_heads": 1,
            "attn_heads_num": 71,
            "hidden_size": 4544,
            "ffn_dim": 18176,
            "vocab_size": 65024,
            "tokenizer_size": 2.7,
            "compiled_graph_size": 24
        },
        "falcon-11b": {
            "size": 11,
            "layers": 60,
            "attn_heads": 8,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 16384,
            "vocab_size": 65024,
            "tokenizer_size": 2.7,
            "compiled_graph_size": 24
        },
        "yi-6b": {
            "size": 6,
            "layers": 32,
            "attn_heads": 4,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5,
            "compiled_graph_size": 36
        },
        "yi-9b": {
            "size": 9,
            "layers": 48,
            "attn_heads": 4,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5,
            "compiled_graph_size": 24
        },
        "mpt-7b": {
            "size": 7,
            "layers": 32,
            "attn_heads": 32,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 16384,
            "vocab_size": 50432,
            "tokenizer_size": 2.1,
            "compiled_graph_size": 13
        },
        "mistral-7b": {
            "size": 7,
            "layers": 32,
            "attn_heads": 8,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 14336,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_size": 24
        }
    }
}