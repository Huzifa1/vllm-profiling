{
    "labels": [
        "deepseek-r1-distill-llama-8b",
        "deepseek-r1-distill-qwen-7b",
        "deepseek-v2-lite-16b",
        "falcon-7b",
        "gpt-oss-20b",
        "llama2-13b-hf",
        "llama2-7b-hf",
        "llama3-3b",
        "qwen-0.5b",
        "qwen-14.3b",
        "qwen-14b",
        "qwen-1.8b",
        "qwen-4b",
        "qwen-7b",
        "yi-6b",
        "yi-9b"
    ],
    "data": {
        "load_weights": [
            2.358,
            2.2239999999999998,
            4.686,
            1.918,
            1.9940000000000002,
            3.554,
            1.796,
            0.86,
            0.14200000000000002,
            4.066,
            3.7960000000000003,
            0.51,
            1.094,
            2.084,
            1.624,
            2.4299999999999997
        ],
        "model_init": [
            0.14,
            0.11000000000000001,
            0.16,
            0.12,
            0.13,
            0.128,
            0.12,
            0.13,
            0.1,
            0.12,
            0.13,
            0.10600000000000001,
            0.122,
            0.12,
            0.118,
            0.136
        ],
        "dynamo_transform_time": [
            4.918,
            4.2700000000000005,
            4.234,
            3.9799999999999995,
            3.558,
            5.9719999999999995,
            4.864,
            4.246,
            3.722,
            4.3420000000000005,
            6.076,
            3.6980000000000004,
            6.146,
            4.968,
            4.994,
            6.996
        ],
        "graph_compile_general_shape": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "graph_compile_cached": [
            3.8031999999999995,
            3.3724000000000003,
            3.1426000000000003,
            3.5338000000000003,
            3.3836,
            4.639799999999999,
            3.6159999999999997,
            3.2998,
            2.6130000000000004,
            2.8858000000000006,
            4.6936,
            2.7318,
            4.553400000000001,
            3.6760000000000006,
            3.7668,
            5.655799999999999
        ],
        "kv_cache_profiling": [
            1.3727999999999998,
            1.5616000000000003,
            1.8773999999999997,
            2.1342,
            1.4144000000000005,
            2.3202,
            1.3819999999999997,
            1.4582000000000015,
            1.367,
            1.454200000000001,
            2.3324,
            1.3762000000000008,
            1.5505999999999993,
            1.4019999999999992,
            1.3712,
            2.0142000000000007
        ],
        "graph_capturing": [
            1.16,
            1.146,
            2.274,
            1.118,
            1.2840000000000003,
            1.434,
            1.0859999999999999,
            0.9960000000000001,
            0.908,
            1.508,
            1.486,
            0.9019999999999999,
            1.278,
            1.114,
            1.1060000000000003,
            1.452
        ],
        "tokenizer_init": [
            0.372,
            0.328,
            0.21000000000000002,
            0.122,
            0.674,
            0.082,
            0.08,
            0.29,
            0.25,
            0.25,
            0.25,
            0.25,
            0.25,
            0.25,
            0.16,
            0.158
        ],
        "total_time": [
            14.123999999999999,
            13.012,
            16.584,
            12.926,
            12.438,
            18.13,
            12.944,
            11.280000000000001,
            9.102000000000002,
            14.626000000000001,
            18.764000000000003,
            9.574,
            14.994,
            13.614,
            13.14,
            18.842000000000002
        ]
    },
    "models_config": {
        "qwen-0.5b": {
            "size": 0.5,
            "layers": 24,
            "attn_heads": 16,
            "attn_heads_num": 16,
            "hidden_size": 1024,
            "ffn_dim": 2816,
            "vocab_size": 151936,
            "tokenizer_size": 6.8
        },
        "qwen-1.8b": {
            "size": 1.8,
            "layers": 24,
            "attn_heads": 16,
            "attn_heads_num": 16,
            "hidden_size": 2048,
            "ffn_dim": 5504,
            "vocab_size": 151936,
            "tokenizer_size": 6.8
        },
        "qwen-4b": {
            "size": 4,
            "layers": 40,
            "attn_heads": 20,
            "attn_heads_num": 20,
            "hidden_size": 2560,
            "ffn_dim": 6912,
            "vocab_size": 151936,
            "tokenizer_size": 6.8
        },
        "qwen-7b": {
            "size": 7,
            "layers": 32,
            "attn_heads": 32,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 151936,
            "tokenizer_size": 6.8
        },
        "qwen-14b": {
            "size": 14,
            "layers": 40,
            "attn_heads": 40,
            "attn_heads_num": 40,
            "hidden_size": 5120,
            "ffn_dim": 13696,
            "vocab_size": 152064,
            "tokenizer_size": 6.8
        },
        "qwen-14.3b": {
            "size": 14.3,
            "layers": 24,
            "attn_heads": 16,
            "attn_heads_num": 16,
            "hidden_size": 2048,
            "ffn_dim": 1408,
            "vocab_size": 152064,
            "tokenizer_size": 6.8,
            "moe": {
                "n_experts": 60,
                "n_active_experts": 4,
                "active_size": 2.7
            }
        },
        "llama2-7b-hf": {
            "size": 7,
            "layers": 32,
            "attn_heads": 32,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 32000,
            "tokenizer_size": 1.8
        },
        "llama2-13b-hf": {
            "size": 13,
            "layers": 40,
            "attn_heads": 40,
            "attn_heads_num": 40,
            "hidden_size": 5120,
            "ffn_dim": 13824,
            "vocab_size": 32000,
            "tokenizer_size": 1.8
        },
        "llama3-3b": {
            "size": 3,
            "layers": 28,
            "attn_heads": 8,
            "attn_heads_num": 24,
            "hidden_size": 3072,
            "ffn_dim": 8192,
            "vocab_size": 128256,
            "tokenizer_size": 8.7
        },
        "falcon-7b": {
            "size": 7,
            "layers": 32,
            "attn_heads": 1,
            "attn_heads_num": 71,
            "hidden_size": 4544,
            "ffn_dim": 18176,
            "vocab_size": 65024,
            "tokenizer_size": 2.7
        },
        "falcon-11b": {
            "size": 11,
            "layers": 60,
            "attn_heads": 8,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 16384,
            "vocab_size": 65024,
            "tokenizer_size": 2.7
        },
        "yi-6b": {
            "size": 6,
            "layers": 32,
            "attn_heads": 4,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5
        },
        "yi-9b": {
            "size": 9,
            "layers": 48,
            "attn_heads": 4,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5
        },
        "mpt-7b": {
            "size": 7,
            "layers": 32,
            "attn_heads": 32,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 16384,
            "vocab_size": 50432,
            "tokenizer_size": 2.1
        },
        "mistral-7b": {
            "size": 7,
            "layers": 32,
            "attn_heads": 8,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 14336,
            "vocab_size": 32000,
            "tokenizer_size": 1.8
        },
        "gpt-oss-20b": {
            "size": 21,
            "layers": 24,
            "attn_heads": 8,
            "attn_heads_num": 64,
            "hidden_size": 2880,
            "ffn_dim": 2880,
            "vocab_size": 201088,
            "tokenizer_size": 27,
            "moe": {
                "n_experts": 32,
                "n_active_experts": 4,
                "active_size": 3.6
            }
        },
        "deepseek-v2-lite-16b": {
            "size": 16,
            "layers": 27,
            "attn_heads": 16,
            "attn_heads_num": 16,
            "hidden_size": 2048,
            "ffn_dim": 1408,
            "vocab_size": 102400,
            "tokenizer_size": 4.4,
            "moe": {
                "n_experts": 64,
                "n_active_experts": 6,
                "active_size": 2.4
            }
        },
        "deepseek-r1-distill-llama-8b": {
            "size": 8,
            "layers": 32,
            "attn_heads": 8,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 14336,
            "vocab_size": 128256,
            "tokenizer_size": 8.7
        },
        "deepseek-r1-distill-qwen-7b": {
            "size": 7,
            "layers": 28,
            "attn_heads": 4,
            "attn_heads_num": 28,
            "hidden_size": 3584,
            "ffn_dim": 18944,
            "vocab_size": 152064,
            "tokenizer_size": 6.8
        },
        "mixtral-46.7b": {
            "size": 46.7,
            "layers": 32,
            "attn_heads": 8,
            "attn_heads_num": 32,
            "hidden_size": 4096,
            "ffn_dim": 14336,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "moe": {
                "n_experts": 8,
                "n_active_experts": 2,
                "active_size": 12.9
            }
        }
    }
}