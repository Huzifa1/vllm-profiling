{
    "labels": [
        "output_model_deepseek-r1-distill-llama-8b.txt",
        "output_model_deepseek-r1-distill-qwen-7b.txt",
        "output_model_deepseek-v2-lite-16b.txt",
        "output_model_falcon-7b.txt",
        "output_model_llama2-13b-hf.txt",
        "output_model_llama2-7b-hf.txt",
        "output_model_llama3-3b.txt",
        "output_model_qwen-0.5b.txt",
        "output_model_qwen-14.3b.txt",
        "output_model_qwen-14b.txt",
        "output_model_qwen-1.8b.txt",
        "output_model_qwen-4b.txt",
        "output_model_qwen-7b.txt",
        "output_model_yi-6b.txt",
        "output_model_yi-9b.txt"
    ],
    "data": {
        "detect_platfrom": [
            0.034,
            0.034,
            0.034,
            0.033,
            0.034,
            0.034,
            0.034,
            0.034,
            0.034,
            0.034,
            0.034,
            0.034,
            0.034,
            0.034,
            0.033
        ],
        "llm_imports": [
            1.36,
            1.358,
            1.408,
            1.356,
            1.369,
            1.403,
            1.354,
            1.369,
            1.366,
            1.362,
            1.361,
            1.474,
            1.367,
            1.402,
            1.367
        ],
        "get_model_info": [
            5.009,
            5.017,
            5.041,
            4.991,
            4.978,
            4.992,
            5.018,
            4.978,
            5.041,
            5.001,
            4.992,
            5.002,
            4.985,
            4.999,
            5.005
        ],
        "worker_init": [
            1.484,
            1.516,
            1.465,
            1.448,
            1.445,
            1.509,
            1.464,
            1.474,
            1.478,
            1.478,
            1.458,
            1.467,
            1.464,
            1.456,
            1.464
        ],
        "framework_bootstrap": [
            7.8870000000000005,
            7.925000000000001,
            7.948,
            7.827999999999999,
            7.8260000000000005,
            7.938000000000001,
            7.869999999999999,
            7.855,
            7.9190000000000005,
            7.875,
            7.845000000000001,
            7.977,
            7.85,
            7.891,
            7.869
        ],
        "load_weights": [
            2.31,
            2.14,
            4.61,
            1.93,
            3.59,
            1.85,
            0.93,
            0.14,
            4.25,
            3.86,
            0.52,
            1.11,
            2.14,
            1.67,
            2.41
        ],
        "model_init": [
            0.16,
            0.13,
            0.18,
            0.12,
            0.12,
            0.12,
            0.16,
            0.11,
            0.12,
            0.14,
            0.12,
            0.14,
            0.13,
            0.12,
            0.14
        ],
        "model_loading": [
            2.482911,
            2.272694,
            4.804181,
            2.054068,
            3.720327,
            1.974176,
            1.0915,
            0.257278,
            4.373942,
            4.006373,
            0.642327,
            1.258504,
            2.277987,
            1.80133,
            2.550226
        ],
        "dynamo_transform_time": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
        ],
        "graph_compile_general_shape": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
        ],
        "graph_compile_cached": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
        ],
        "torch.compile": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
        ],
        "kv_cache_profiling": [
            0.69,
            0.72,
            0.94,
            0.66,
            0.76,
            0.71,
            0.66,
            0.66,
            0.93,
            0.78,
            0.69,
            0.69,
            0.73,
            0.7,
            0.74
        ],
        "graph_capturing": [
            1.49,
            1.44,
            2.52,
            1.18,
            1.84,
            1.45,
            1.37,
            1.27,
            1.88,
            1.93,
            1.24,
            1.79,
            1.48,
            1.46,
            2.0
        ],
        "init_engine": [
            2.61,
            2.59,
            3.9,
            2.27,
            3.03,
            2.59,
            2.46,
            2.35,
            3.24,
            3.15,
            2.36,
            2.9,
            2.64,
            2.6,
            3.17
        ],
        "tokenizer_init": [
            0.37,
            0.32,
            0.22,
            0.12,
            0.08,
            0.08,
            0.29,
            0.25,
            0.25,
            0.24,
            0.25,
            0.25,
            0.25,
            0.16,
            0.16
        ],
        "total_time": [
            13.349910999999999,
            13.107694,
            16.872180999999998,
            12.272067999999999,
            14.656327000000001,
            12.582176,
            11.711499999999997,
            10.712278,
            15.782942,
            15.271373,
            11.097327,
            12.385504000000001,
            13.017987,
            12.45233,
            13.749226
        ],
        "actual_total_time": [
            14.52,
            14.246,
            17.853,
            13.848,
            15.521,
            13.443,
            12.803,
            11.748,
            16.83,
            16.314,
            12.127,
            13.413,
            14.055,
            13.413,
            14.712
        ]
    }
}