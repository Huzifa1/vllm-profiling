{
    "env": {
        "VLLM_USE_V1": 1,
        "CUDA_VISIBLE_DEVICES": "0",
        "VLLM_LOGGING_LEVEL": "DEBUG"
    },
    "params": {
        "model": [
            "/local/huzaifa/workspace/models/falcon-7b",
            "/local/huzaifa/workspace/models/llama2-13b-hf",
            "/local/huzaifa/workspace/models/llama2-7b-hf",
            "/local/huzaifa/workspace/models/llama3-3b",
            "/local/huzaifa/workspace/models/qwen-0.5b",
            "/local/huzaifa/workspace/models/qwen-14b",
            "/local/huzaifa/workspace/models/qwen-14.3b",
            "/local/huzaifa/workspace/models/qwen-1.8b",
            "/local/huzaifa/workspace/models/qwen-4b",
            "/local/huzaifa/workspace/models/qwen-7b",
            "/local/huzaifa/workspace/models/yi-6b",
            "/local/huzaifa/workspace/models/yi-9b",
            "/local/huzaifa/workspace/models/gpt-oss-20b",
            "/local/huzaifa/workspace/models/deepseek-v2-lite",
            "/local/huzaifa/workspace/models/deepseek-r1-distill-llama-8b",
            "/local/huzaifa/workspace/models/deepseek-r1-distill-qwen-7b"
        ]
    },
    "notes": [
        "gpt-oss-20b was run with 35 different batch size explicitly, to avoid the default overriding to 83",
        "mixtral-46.7b was run with --gpu-memory-utilization 0.95 and --max-model-len 7728 to avoid OOM."
    ]
}
