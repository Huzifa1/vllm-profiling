{
    "labels": [
        "falcon-7b",
        "llama2-13b-hf",
        "llama2-7b-hf",
        "llama3-3b",
        "qwen-0.5b",
        "qwen-14b",
        "qwen-1.8b",
        "qwen-4b",
        "qwen-7b",
        "yi-6b",
        "yi-9b"
    ],
    "data": {
        "load_weights": [
            1.8780000000000001,
            3.5060000000000002,
            1.81,
            0.866,
            0.14400000000000002,
            3.772,
            0.506,
            1.0740000000000003,
            2.098,
            1.64,
            2.4
        ],
        "model_init": [
            0.11000000000000001,
            0.12,
            0.11000000000000001,
            0.13,
            0.092,
            0.12,
            0.1,
            0.12,
            0.11000000000000001,
            0.11000000000000001,
            0.13
        ],
        "dynamo_transform_time": [
            4.023999999999999,
            5.932,
            4.955999999999999,
            4.152,
            3.71,
            6.16,
            3.752,
            6.000000000000001,
            4.984,
            4.906000000000001,
            6.984
        ],
        "graph_compile_general_shape": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "graph_compile_cached": [
            3.4396,
            4.471400000000001,
            3.5135999999999994,
            3.2598,
            2.6206000000000005,
            4.5124,
            2.6654,
            4.5526,
            3.6438,
            3.7769999999999997,
            5.6638
        ],
        "kv_cache_profiling": [
            1.6264000000000003,
            1.7706,
            1.4723999999999986,
            1.5261999999999993,
            1.4053999999999993,
            1.7916000000000007,
            1.4126000000000003,
            1.6213999999999977,
            1.4941999999999993,
            1.4570000000000007,
            2.148199999999999
        ],
        "graph_capturing": [
            1.4679999999999997,
            2.316,
            1.462,
            0.986,
            0.63,
            2.332,
            0.712,
            1.182,
            1.4920000000000002,
            1.3760000000000001,
            1.9619999999999997
        ],
        "tokenizer_init": [
            0.12,
            0.08,
            0.08,
            0.28400000000000003,
            0.248,
            0.246,
            0.248,
            0.25,
            0.248,
            0.154,
            0.15
        ],
        "total_time": [
            12.665999999999999,
            18.196,
            13.403999999999996,
            11.204,
            8.85,
            18.933999999999997,
            9.395999999999999,
            14.8,
            14.07,
            13.42,
            19.438
        ]
    },
    "models_config": {
        "qwen-0.5b": {
            "size": 0.5,
            "layers": 24,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 1024,
            "ffn_dim": 2816,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 19
        },
        "qwen-1.8b": {
            "size": 1.8,
            "layers": 24,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 2048,
            "ffn_dim": 5504,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 20
        },
        "qwen-4b": {
            "size": 4,
            "layers": 40,
            "num_key_value_heads": 20,
            "num_attention_heads": 20,
            "hidden_size": 2560,
            "ffn_dim": 6912,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 20
        },
        "qwen-7b": {
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 32,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 33
        },
        "qwen-14b": {
            "size": 14,
            "layers": 40,
            "num_key_value_heads": 40,
            "num_attention_heads": 40,
            "hidden_size": 5120,
            "ffn_dim": 13696,
            "vocab_size": 152064,
            "tokenizer_size": 6.8,
            "compiled_graph_size": 33
        },
        "llama2-7b-hf": {
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 32,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_size": 20
        },
        "llama2-13b-hf": {
            "size": 13,
            "layers": 40,
            "num_key_value_heads": 40,
            "num_attention_heads": 40,
            "hidden_size": 5120,
            "ffn_dim": 13824,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_size": 20
        },
        "llama3-3b": {
            "size": 3,
            "layers": 28,
            "num_key_value_heads": 8,
            "num_attention_heads": 24,
            "hidden_size": 3072,
            "ffn_dim": 8192,
            "vocab_size": 128256,
            "tokenizer_size": 8.7,
            "compiled_graph_size": 24
        },
        "falcon-7b": {
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 1,
            "num_attention_heads": 71,
            "hidden_size": 4544,
            "ffn_dim": 18176,
            "vocab_size": 65024,
            "tokenizer_size": 2.7,
            "compiled_graph_size": 24
        },
        "falcon-11b": {
            "size": 11,
            "layers": 60,
            "num_key_value_heads": 8,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 16384,
            "vocab_size": 65024,
            "tokenizer_size": 2.7,
            "compiled_graph_size": 24
        },
        "yi-6b": {
            "size": 6,
            "layers": 32,
            "num_key_value_heads": 4,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5,
            "compiled_graph_size": 36
        },
        "yi-9b": {
            "size": 9,
            "layers": 48,
            "num_key_value_heads": 4,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5,
            "compiled_graph_size": 24
        },
        "mpt-7b": {
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 32,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 16384,
            "vocab_size": 50432,
            "tokenizer_size": 2.1,
            "compiled_graph_size": 13
        },
        "mistral-7b": {
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 8,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 14336,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_size": 24
        }
    }
}