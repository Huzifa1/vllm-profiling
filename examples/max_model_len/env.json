{
    "env": {
        "VLLM_USE_V1": 1,
        "CUDA_VISIBLE_DEVICES": "0",
        "VLLM_LOGGING_LEVEL": "DEBUG"
    },
    "params": {
        "model": [
            "/models/llama2-13b-hf",
            "/models/qwen-4b"
        ],
        "max-model-len": [256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
    },
    "notes": "Llama2-13b supports up to 4096, Qwen-4b supports up to 32768"
}