DEBUG 08-05 15:27:43 [__init__.py:31] No plugins for group vllm.platform_plugins found.
DEBUG 08-05 15:27:43 [__init__.py:35] Checking if TPU platform is available.
DEBUG 08-05 15:27:43 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'
DEBUG 08-05 15:27:43 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 08-05 15:27:43 [__init__.py:72] Confirmed CUDA platform is available.
DEBUG 08-05 15:27:43 [__init__.py:100] Checking if ROCm platform is available.
DEBUG 08-05 15:27:43 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 08-05 15:27:43 [__init__.py:121] Checking if HPU platform is available.
DEBUG 08-05 15:27:43 [__init__.py:128] HPU platform is not available because habana_frameworks is not found.
DEBUG 08-05 15:27:43 [__init__.py:138] Checking if XPU platform is available.
DEBUG 08-05 15:27:43 [__init__.py:148] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 08-05 15:27:43 [__init__.py:155] Checking if CPU platform is available.
DEBUG 08-05 15:27:43 [__init__.py:177] Checking if Neuron platform is available.
DEBUG 08-05 15:27:43 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 08-05 15:27:43 [__init__.py:72] Confirmed CUDA platform is available.
INFO 08-05 15:27:43 [__init__.py:244] Automatically detected platform cuda.
DEBUG 08-05 15:27:47 [__init__.py:39] Available plugins for group vllm.general_plugins:
DEBUG 08-05 15:27:47 [__init__.py:41] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 08-05 15:27:47 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 08-05 15:27:53 [config.py:841] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 08-05 15:27:53 [config.py:1472] Using max model len 32768
INFO 08-05 15:27:54 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 08-05 15:27:54 [llm_engine.py:152] Enabling multiprocessing for LLMEngine.
INFO 08-05 15:27:54 [llm_engine.py:93] NEW: Init Tokenizer took 0.41 seconds
WARNING 08-05 15:27:54 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
DEBUG 08-05 15:27:58 [__init__.py:31] No plugins for group vllm.platform_plugins found.
DEBUG 08-05 15:27:58 [__init__.py:35] Checking if TPU platform is available.
DEBUG 08-05 15:27:58 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'
DEBUG 08-05 15:27:58 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 08-05 15:27:58 [__init__.py:72] Confirmed CUDA platform is available.
DEBUG 08-05 15:27:58 [__init__.py:100] Checking if ROCm platform is available.
DEBUG 08-05 15:27:58 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 08-05 15:27:58 [__init__.py:121] Checking if HPU platform is available.
DEBUG 08-05 15:27:58 [__init__.py:128] HPU platform is not available because habana_frameworks is not found.
DEBUG 08-05 15:27:58 [__init__.py:138] Checking if XPU platform is available.
DEBUG 08-05 15:27:58 [__init__.py:148] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 08-05 15:27:58 [__init__.py:155] Checking if CPU platform is available.
DEBUG 08-05 15:27:58 [__init__.py:177] Checking if Neuron platform is available.
DEBUG 08-05 15:27:58 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 08-05 15:27:58 [__init__.py:72] Confirmed CUDA platform is available.
INFO 08-05 15:27:58 [__init__.py:244] Automatically detected platform cuda.
INFO 08-05 15:28:00 [core.py:526] Waiting for init message from front-end.
DEBUG 08-05 15:28:00 [utils.py:545] HELLO from local core engine process 0.
DEBUG 08-05 15:28:00 [core.py:534] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/537c2a38-d50f-4546-ba40-b9c69d473b85'], outputs=['ipc:///tmp/f5efb4b2-6fcc-40bb-88a7-c5bc857111fc'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, 'data_parallel_size': 1})
DEBUG 08-05 15:28:00 [__init__.py:39] Available plugins for group vllm.general_plugins:
DEBUG 08-05 15:28:00 [__init__.py:41] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 08-05 15:28:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 08-05 15:28:00 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/models/qwen-14b', speculative_config=None, tokenizer='/models/qwen-14b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/models/qwen-14b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
DEBUG 08-05 15:28:00 [decorators.py:110] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 08-05 15:28:00 [decorators.py:110] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 08-05 15:28:01 [__init__.py:2802] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x72b170baf710>
DEBUG 08-05 15:28:01 [config.py:4834] enabled custom ops: Counter()
DEBUG 08-05 15:28:01 [config.py:4836] disabled custom ops: Counter()
DEBUG 08-05 15:28:01 [parallel_state.py:919] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.6.170:60921 backend=nccl
DEBUG 08-05 15:28:01 [parallel_state.py:970] Detected 1 nodes in the distributed environment
INFO 08-05 15:28:01 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 08-05 15:28:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
DEBUG 08-05 15:28:01 [config.py:4834] enabled custom ops: Counter()
DEBUG 08-05 15:28:01 [config.py:4836] disabled custom ops: Counter()
INFO 08-05 15:28:01 [gpu_model_runner.py:1770] Starting to load model /models/qwen-14b...
INFO 08-05 15:28:01 [gpu_model_runner.py:1775] Loading model from scratch...
INFO 08-05 15:28:01 [cuda.py:284] Using Flash Attention backend on V1 engine.
DEBUG 08-05 15:28:02 [backends.py:39] Using InductorAdaptor
DEBUG 08-05 15:28:02 [config.py:4834] enabled custom ops: Counter()
DEBUG 08-05 15:28:02 [config.py:4836] disabled custom ops: Counter({'rms_norm': 81, 'silu_and_mul': 40, 'rotary_embedding': 1})
INFO 08-05 15:28:02 [base_loader.py:45] NEW: Initializing Model took 0.23 seconds
DEBUG 08-05 15:28:10 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 08-05 15:28:20 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 08-05 15:28:30 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 08-05 15:28:32 [utils.py:183] Loaded weight lm_head.weight with shape torch.Size([152064, 5120])
INFO 08-05 15:28:32 [default_loader.py:272] Loading weights took 30.40 seconds
INFO 08-05 15:28:32 [gpu_model_runner.py:1801] Model loading took 26.4278 GiB and 30.644763 seconds
DEBUG 08-05 15:28:33 [decorators.py:204] Start compiling function <code object forward at 0x411532a0, file "/home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 337>
DEBUG 08-05 15:28:40 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 08-05 15:28:42 [backends.py:461] Traced files (to be considered for compilation cache):
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/torch/nn/modules/container.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/torch/nn/modules/module.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/attention/layer.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/distributed/communication_op.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py
DEBUG 08-05 15:28:42 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/platforms/interface.py
INFO 08-05 15:28:43 [backends.py:508] Using cache directory: /home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/backbone for vLLM's torch.compile
INFO 08-05 15:28:43 [backends.py:519] Dynamo bytecode transform time: 10.54 s
DEBUG 08-05 15:28:44 [backends.py:123] Directly load the 0-th graph for shape None from inductor via handle ('f6zdxmc5m5iwfyoo3k4upxq5kmftdc2xlzh3qyjs37m3q37n6gfh', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/jp/cjpqpcngsp4ib3mgwrpmkoe2grm6y2z5wrc7kvrivtrrcjednor4.py')
DEBUG 08-05 15:28:44 [backends.py:123] Directly load the 1-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:44 [backends.py:123] Directly load the 2-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:44 [backends.py:123] Directly load the 3-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:44 [backends.py:123] Directly load the 4-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:45 [backends.py:123] Directly load the 5-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:45 [backends.py:123] Directly load the 6-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:45 [backends.py:123] Directly load the 7-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:45 [backends.py:123] Directly load the 8-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:45 [backends.py:123] Directly load the 9-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:46 [backends.py:123] Directly load the 10-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:46 [backends.py:123] Directly load the 11-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:46 [backends.py:123] Directly load the 12-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:46 [backends.py:123] Directly load the 13-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:47 [backends.py:123] Directly load the 14-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:47 [backends.py:123] Directly load the 15-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:47 [backends.py:123] Directly load the 16-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:47 [backends.py:123] Directly load the 17-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:48 [backends.py:123] Directly load the 18-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:48 [backends.py:123] Directly load the 19-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:48 [backends.py:123] Directly load the 20-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:48 [backends.py:123] Directly load the 21-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:48 [backends.py:123] Directly load the 22-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:49 [backends.py:123] Directly load the 23-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:49 [backends.py:123] Directly load the 24-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:49 [backends.py:123] Directly load the 25-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:49 [backends.py:123] Directly load the 26-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:50 [backends.py:123] Directly load the 27-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:50 [backends.py:123] Directly load the 28-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:50 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 08-05 15:28:50 [backends.py:123] Directly load the 29-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:51 [backends.py:123] Directly load the 30-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:51 [backends.py:123] Directly load the 31-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:51 [backends.py:123] Directly load the 32-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:51 [backends.py:123] Directly load the 33-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:52 [backends.py:123] Directly load the 34-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:52 [backends.py:123] Directly load the 35-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:52 [backends.py:123] Directly load the 36-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:52 [backends.py:123] Directly load the 37-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:52 [backends.py:123] Directly load the 38-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:53 [backends.py:123] Directly load the 39-th graph for shape None from inductor via handle ('f5g3o4hfacwysxnyz7ew76pfgykszc7yuwih4czaxtbtzw4vnxpv', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/52/c52limdzrj7oa4aw6l65ygiwj7coj2h76popxfwjwcbure2676aw.py')
DEBUG 08-05 15:28:53 [backends.py:123] Directly load the 40-th graph for shape None from inductor via handle ('ff36wjgwksti534kuggeneb7zgjdfr6ybpgbmvldj3em4zzpugy5', '/home/admin/.cache/vllm/torch_compile_cache/258701262b/rank_0_0/inductor_cache/dc/cdcinuao5fdrseckbbecgjlj4rivwwngvbg56343jp4eacb436mp.py')
INFO 08-05 15:28:53 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 9.530 s
INFO 08-05 15:28:57 [monitor.py:34] torch.compile takes 10.54 s in total
DEBUG 08-05 15:28:58 [gpu_worker.py:226] Initial free memory: 38.65 GiB, free memory: 12.10 GiB, requested GPU memory: 35.54 GiB
DEBUG 08-05 15:28:58 [gpu_worker.py:231] Memory profiling takes 25.03 seconds. Total non KV cache memory: 27.24GiB; torch peak memory increase: 0.72GiB; non-torch forward increase memory: 0.09GiB; weights memory: 26.43GiB.
INFO 08-05 15:28:58 [gpu_worker.py:232] Available KV cache memory: 8.31 GiB
ERROR 08-05 15:28:58 [core.py:586] EngineCore failed to start.
ERROR 08-05 15:28:58 [core.py:586] Traceback (most recent call last):
ERROR 08-05 15:28:58 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 577, in run_engine_core
ERROR 08-05 15:28:58 [core.py:586]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 08-05 15:28:58 [core.py:586]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-05 15:28:58 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 404, in __init__
ERROR 08-05 15:28:58 [core.py:586]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 08-05 15:28:58 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 82, in __init__
ERROR 08-05 15:28:58 [core.py:586]     self._initialize_kv_caches(vllm_config)
ERROR 08-05 15:28:58 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 146, in _initialize_kv_caches
ERROR 08-05 15:28:58 [core.py:586]     kv_cache_configs = [
ERROR 08-05 15:28:58 [core.py:586]                        ^
ERROR 08-05 15:28:58 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 147, in <listcomp>
ERROR 08-05 15:28:58 [core.py:586]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
ERROR 08-05 15:28:58 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 943, in get_kv_cache_config
ERROR 08-05 15:28:58 [core.py:586]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
ERROR 08-05 15:28:58 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 572, in check_enough_kv_cache_memory
ERROR 08-05 15:28:58 [core.py:586]     raise ValueError(
ERROR 08-05 15:28:58 [core.py:586] ValueError: To serve at least one request with the models's max seq len (32768), (25.00 GiB KV cache is needed, which is larger than the available KV cache memory (8.31 GiB). Based on the available memory, the estimated maximum model length is 10880. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
