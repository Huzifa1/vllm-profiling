DEBUG 07-25 08:21:11 [__init__.py:31] No plugins for group vllm.platform_plugins found.
DEBUG 07-25 08:21:11 [__init__.py:35] Checking if TPU platform is available.
DEBUG 07-25 08:21:11 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'
DEBUG 07-25 08:21:11 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 07-25 08:21:11 [__init__.py:72] Confirmed CUDA platform is available.
DEBUG 07-25 08:21:11 [__init__.py:100] Checking if ROCm platform is available.
DEBUG 07-25 08:21:11 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 07-25 08:21:11 [__init__.py:121] Checking if HPU platform is available.
DEBUG 07-25 08:21:11 [__init__.py:128] HPU platform is not available because habana_frameworks is not found.
DEBUG 07-25 08:21:11 [__init__.py:138] Checking if XPU platform is available.
DEBUG 07-25 08:21:11 [__init__.py:148] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 07-25 08:21:11 [__init__.py:155] Checking if CPU platform is available.
DEBUG 07-25 08:21:11 [__init__.py:177] Checking if Neuron platform is available.
DEBUG 07-25 08:21:11 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 07-25 08:21:11 [__init__.py:72] Confirmed CUDA platform is available.
INFO 07-25 08:21:11 [__init__.py:244] Automatically detected platform cuda.
DEBUG 07-25 08:21:15 [__init__.py:39] Available plugins for group vllm.general_plugins:
DEBUG 07-25 08:21:15 [__init__.py:41] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 07-25 08:21:15 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-25 08:21:21 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 07-25 08:21:21 [config.py:1472] Using max model len 32768
INFO 07-25 08:21:22 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 07-25 08:21:22 [llm_engine.py:155] Enabling multiprocessing for LLMEngine.
INFO 07-25 08:21:22 [llm_engine.py:95] HUZ: Init Tokenizer took 0.39 seconds
WARNING 07-25 08:21:22 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
DEBUG 07-25 08:21:26 [__init__.py:31] No plugins for group vllm.platform_plugins found.
DEBUG 07-25 08:21:26 [__init__.py:35] Checking if TPU platform is available.
DEBUG 07-25 08:21:26 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'
DEBUG 07-25 08:21:26 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 07-25 08:21:26 [__init__.py:72] Confirmed CUDA platform is available.
DEBUG 07-25 08:21:26 [__init__.py:100] Checking if ROCm platform is available.
DEBUG 07-25 08:21:26 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 07-25 08:21:26 [__init__.py:121] Checking if HPU platform is available.
DEBUG 07-25 08:21:26 [__init__.py:128] HPU platform is not available because habana_frameworks is not found.
DEBUG 07-25 08:21:26 [__init__.py:138] Checking if XPU platform is available.
DEBUG 07-25 08:21:26 [__init__.py:148] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 07-25 08:21:26 [__init__.py:155] Checking if CPU platform is available.
DEBUG 07-25 08:21:26 [__init__.py:177] Checking if Neuron platform is available.
DEBUG 07-25 08:21:26 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 07-25 08:21:26 [__init__.py:72] Confirmed CUDA platform is available.
INFO 07-25 08:21:26 [__init__.py:244] Automatically detected platform cuda.
INFO 07-25 08:21:28 [core.py:526] Waiting for init message from front-end.
DEBUG 07-25 08:21:28 [utils.py:545] HELLO from local core engine process 0.
DEBUG 07-25 08:21:28 [core.py:534] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/1cfa75bc-ddea-4597-ac58-b5e253fb115b'], outputs=['ipc:///tmp/166a4978-360c-486b-b05f-c68cc984b1c8'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, 'data_parallel_size': 1})
DEBUG 07-25 08:21:28 [__init__.py:39] Available plugins for group vllm.general_plugins:
DEBUG 07-25 08:21:28 [__init__.py:41] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 07-25 08:21:28 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-25 08:21:28 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/models/qwen-14b', speculative_config=None, tokenizer='/models/qwen-14b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/models/qwen-14b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
DEBUG 07-25 08:21:29 [decorators.py:110] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 07-25 08:21:29 [decorators.py:110] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 07-25 08:21:29 [__init__.py:2802] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7961cb113790>
DEBUG 07-25 08:21:29 [config.py:4834] enabled custom ops: Counter()
DEBUG 07-25 08:21:29 [config.py:4836] disabled custom ops: Counter()
DEBUG 07-25 08:21:29 [parallel_state.py:919] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.6.170:60655 backend=nccl
DEBUG 07-25 08:21:29 [parallel_state.py:970] Detected 1 nodes in the distributed environment
INFO 07-25 08:21:29 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 07-25 08:21:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
DEBUG 07-25 08:21:29 [config.py:4834] enabled custom ops: Counter()
DEBUG 07-25 08:21:29 [config.py:4836] disabled custom ops: Counter()
INFO 07-25 08:21:29 [gpu_model_runner.py:1770] Starting to load model /models/qwen-14b...
INFO 07-25 08:21:30 [gpu_model_runner.py:1775] Loading model from scratch...
INFO 07-25 08:21:30 [cuda.py:284] Using Flash Attention backend on V1 engine.
DEBUG 07-25 08:21:30 [backends.py:39] Using InductorAdaptor
DEBUG 07-25 08:21:30 [config.py:4834] enabled custom ops: Counter()
DEBUG 07-25 08:21:30 [config.py:4836] disabled custom ops: Counter({'rms_norm': 81, 'silu_and_mul': 40, 'rotary_embedding': 1})
INFO 07-25 08:21:30 [base_loader.py:44] HUZ: Initializing Model took 0.24 seconds
DEBUG 07-25 08:21:38 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 07-25 08:21:48 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 07-25 08:21:58 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 07-25 08:22:04 [utils.py:183] Loaded weight lm_head.weight with shape torch.Size([152064, 5120])
INFO 07-25 08:22:04 [default_loader.py:272] Loading weights took 34.37 seconds
INFO 07-25 08:22:05 [gpu_model_runner.py:1801] Model loading took 26.4278 GiB and 34.631681 seconds
DEBUG 07-25 08:22:05 [decorators.py:204] Start compiling function <code object forward at 0x5d4ac6070700, file "/home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 337>
DEBUG 07-25 08:22:08 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 07-25 08:22:15 [backends.py:461] Traced files (to be considered for compilation cache):
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/attention/layer.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/distributed/communication_op.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py
DEBUG 07-25 08:22:15 [backends.py:461] /home/admin/.local/lib/python3.11/site-packages/vllm/platforms/interface.py
DEBUG 07-25 08:22:15 [backends.py:461] /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
DEBUG 07-25 08:22:15 [backends.py:461] /opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py
DEBUG 07-25 08:22:15 [backends.py:461] /opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py
INFO 07-25 08:22:16 [backends.py:508] Using cache directory: /home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/backbone for vLLM's torch.compile
INFO 07-25 08:22:16 [backends.py:519] Dynamo bytecode transform time: 10.96 s
DEBUG 07-25 08:22:16 [backends.py:123] Directly load the 0-th graph for shape None from inductor via handle ('fdudjbpdv5xve65wiu4kf2t7e55v33ozqnjw3ia3sgcv26xvc65z', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/eo/ceokn4jiccsktaakwdlsdkfzrpgmyff63qx45gj2xsjczedcj7yg.py')
DEBUG 07-25 08:22:17 [backends.py:123] Directly load the 1-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:17 [backends.py:123] Directly load the 2-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:17 [backends.py:123] Directly load the 3-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:17 [backends.py:123] Directly load the 4-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:17 [backends.py:123] Directly load the 5-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:18 [backends.py:123] Directly load the 6-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:18 [backends.py:123] Directly load the 7-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:18 [backends.py:123] Directly load the 8-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:18 [backends.py:123] Directly load the 9-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:18 [backends.py:123] Directly load the 10-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:18 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 07-25 08:22:19 [backends.py:123] Directly load the 11-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:19 [backends.py:123] Directly load the 12-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:19 [backends.py:123] Directly load the 13-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:19 [backends.py:123] Directly load the 14-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:20 [backends.py:123] Directly load the 15-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:20 [backends.py:123] Directly load the 16-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:20 [backends.py:123] Directly load the 17-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:20 [backends.py:123] Directly load the 18-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:21 [backends.py:123] Directly load the 19-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:21 [backends.py:123] Directly load the 20-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:21 [backends.py:123] Directly load the 21-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:21 [backends.py:123] Directly load the 22-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:21 [backends.py:123] Directly load the 23-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:22 [backends.py:123] Directly load the 24-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:22 [backends.py:123] Directly load the 25-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:22 [backends.py:123] Directly load the 26-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:22 [backends.py:123] Directly load the 27-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:22 [backends.py:123] Directly load the 28-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:23 [backends.py:123] Directly load the 29-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:23 [backends.py:123] Directly load the 30-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:23 [backends.py:123] Directly load the 31-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:23 [backends.py:123] Directly load the 32-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:24 [backends.py:123] Directly load the 33-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:24 [backends.py:123] Directly load the 34-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:24 [backends.py:123] Directly load the 35-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:25 [backends.py:123] Directly load the 36-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:25 [backends.py:123] Directly load the 37-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:25 [backends.py:123] Directly load the 38-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:25 [backends.py:123] Directly load the 39-th graph for shape None from inductor via handle ('fpncgkswoqg437bs67vcj2cudryzk3thks345xcnsuqjiirgxqdd', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/iw/ciwudzfvvg6wi3mv2rozmkgibqz6vymucdxd4madbk2lsz3lbd2l.py')
DEBUG 07-25 08:22:26 [backends.py:123] Directly load the 40-th graph for shape None from inductor via handle ('fuqkms7ieur73axn4uxb7toa744mzlbfk7x2kqydrs6mjoklrje3', '/home/admin/.cache/vllm/torch_compile_cache/016adaf83b/rank_0_0/inductor_cache/dn/cdnuj7tqc3cnb6seaxthx46qzl67whno2n6gbnvpr3kiuv57xofg.py')
INFO 07-25 08:22:26 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 9.405 s
DEBUG 07-25 08:22:28 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.
INFO 07-25 08:22:29 [monitor.py:34] torch.compile takes 10.96 s in total
DEBUG 07-25 08:22:30 [gpu_worker.py:226] Initial free memory: 38.66 GiB, free memory: 12.10 GiB, requested GPU memory: 35.54 GiB
DEBUG 07-25 08:22:30 [gpu_worker.py:231] Memory profiling takes 25.43 seconds. Total non KV cache memory: 27.24GiB; torch peak memory increase: 0.72GiB; non-torch forward increase memory: 0.09GiB; weights memory: 26.43GiB.
INFO 07-25 08:22:30 [gpu_worker.py:232] Available KV cache memory: 8.31 GiB
ERROR 07-25 08:22:30 [core.py:586] EngineCore failed to start.
ERROR 07-25 08:22:30 [core.py:586] Traceback (most recent call last):
ERROR 07-25 08:22:30 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 577, in run_engine_core
ERROR 07-25 08:22:30 [core.py:586]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 07-25 08:22:30 [core.py:586]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-25 08:22:30 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 404, in __init__
ERROR 07-25 08:22:30 [core.py:586]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 07-25 08:22:30 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 82, in __init__
ERROR 07-25 08:22:30 [core.py:586]     self._initialize_kv_caches(vllm_config)
ERROR 07-25 08:22:30 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 146, in _initialize_kv_caches
ERROR 07-25 08:22:30 [core.py:586]     kv_cache_configs = [
ERROR 07-25 08:22:30 [core.py:586]                        ^
ERROR 07-25 08:22:30 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 147, in <listcomp>
ERROR 07-25 08:22:30 [core.py:586]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
ERROR 07-25 08:22:30 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 943, in get_kv_cache_config
ERROR 07-25 08:22:30 [core.py:586]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
ERROR 07-25 08:22:30 [core.py:586]   File "/home/admin/.local/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 572, in check_enough_kv_cache_memory
ERROR 07-25 08:22:30 [core.py:586]     raise ValueError(
ERROR 07-25 08:22:30 [core.py:586] ValueError: To serve at least one request with the models's max seq len (32768), (25.00 GiB KV cache is needed, which is larger than the available KV cache memory (8.31 GiB). Based on the available memory, the estimated maximum model length is 10880. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
