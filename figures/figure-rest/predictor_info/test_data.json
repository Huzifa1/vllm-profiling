{
    "train": [
        {
            "label": "deepseek-r1-distill-llama-8b",
            "time": 52.7678892,
            "batch_size": 1,
            "size": 8,
            "layers": 32,
            "num_key_value_heads": 8,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 14336,
            "vocab_size": 128256,
            "tokenizer_size": 8.7,
            "compiled_graph_sizes": 1121
        },
        {
            "label": "deepseek-r1-distill-qwen-7b",
            "time": 50.271145,
            "batch_size": 1,
            "size": 7,
            "layers": 28,
            "num_key_value_heads": 4,
            "num_attention_heads": 28,
            "hidden_size": 3584,
            "ffn_dim": 18944,
            "vocab_size": 152064,
            "tokenizer_size": 6.8,
            "compiled_graph_sizes": 984
        },
        {
            "label": "deepseek-v2-lite-16b",
            "time": 80.00542980000002,
            "batch_size": 1,
            "size": 16,
            "layers": 27,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 2048,
            "ffn_dim": 1408,
            "vocab_size": 102400,
            "tokenizer_size": 4.4,
            "moe": {
                "n_experts": 64,
                "n_active_experts": 6,
                "active_size": 2.4
            },
            "compiled_graph_sizes": 1060
        },
        {
            "label": "falcon-7b",
            "time": 47.284372000000005,
            "batch_size": 1,
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 1,
            "num_attention_heads": 71,
            "hidden_size": 4544,
            "ffn_dim": 18176,
            "vocab_size": 65024,
            "tokenizer_size": 2.7,
            "compiled_graph_sizes": 905
        },
        {
            "label": "gpt-oss-20b",
            "time": 49.6720222,
            "batch_size": 1,
            "size": 21,
            "layers": 24,
            "num_key_value_heads": 8,
            "num_attention_heads": 64,
            "hidden_size": 2880,
            "ffn_dim": 2880,
            "vocab_size": 201088,
            "tokenizer_size": 27,
            "moe": {
                "n_experts": 32,
                "n_active_experts": 4,
                "active_size": 3.6
            },
            "compiled_graph_sizes": 917
        },
        {
            "label": "granite3.3-8b-instruct",
            "time": 55.541821399999996,
            "batch_size": 1,
            "size": 8,
            "layers": 40,
            "num_key_value_heads": 8,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 12800,
            "vocab_size": 49159,
            "tokenizer_size": 3.4,
            "compiled_graph_sizes": 1374
        },
        {
            "label": "granite4.0-h-32b",
            "time": 28.545119600000003,
            "batch_size": 1,
            "size": 32,
            "layers": 40,
            "num_key_value_heads": 8,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 1536,
            "vocab_size": 100352,
            "tokenizer_size": 6.9,
            "moe": {
                "n_experts": 72,
                "n_active_experts": 10,
                "active_size": 9
            },
            "compiled_graph_sizes": 865
        },
        {
            "label": "granite4.0-h-3b",
            "time": 18.4126962,
            "batch_size": 1,
            "size": 3,
            "layers": 40,
            "num_key_value_heads": 8,
            "num_attention_heads": 32,
            "hidden_size": 2048,
            "ffn_dim": 8192,
            "vocab_size": 100352,
            "tokenizer_size": 6.9,
            "compiled_graph_sizes": 810
        },
        {
            "label": "llama2-13b-hf",
            "time": 73.728368,
            "batch_size": 1,
            "size": 13,
            "layers": 40,
            "num_key_value_heads": 40,
            "num_attention_heads": 40,
            "hidden_size": 5120,
            "ffn_dim": 13824,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_sizes": 1253
        },
        {
            "label": "llama2-7b-hf",
            "time": 47.353617,
            "batch_size": 1,
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 32,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_sizes": 995
        },
        {
            "label": "qwen-0.5b",
            "time": 21.2611968,
            "batch_size": 1,
            "size": 0.5,
            "layers": 24,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 1024,
            "ffn_dim": 2816,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_sizes": 695
        },
        {
            "label": "qwen-14.3b",
            "time": 74.1918876,
            "batch_size": 1,
            "size": 14.3,
            "layers": 24,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 2048,
            "ffn_dim": 1408,
            "vocab_size": 152064,
            "tokenizer_size": 6.8,
            "moe": {
                "n_experts": 60,
                "n_active_experts": 4,
                "active_size": 2.7
            },
            "compiled_graph_sizes": 808
        },
        {
            "label": "qwen-14b",
            "time": 79.12310919999999,
            "batch_size": 1,
            "size": 14,
            "layers": 40,
            "num_key_value_heads": 40,
            "num_attention_heads": 40,
            "hidden_size": 5120,
            "ffn_dim": 13696,
            "vocab_size": 152064,
            "tokenizer_size": 6.8,
            "compiled_graph_sizes": 1262
        },
        {
            "label": "qwen-1.8b",
            "time": 26.368855599999996,
            "batch_size": 1,
            "size": 1.8,
            "layers": 24,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 2048,
            "ffn_dim": 5504,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_sizes": 747
        },
        {
            "label": "qwen-4b",
            "time": 39.795650800000004,
            "batch_size": 1,
            "size": 4,
            "layers": 40,
            "num_key_value_heads": 20,
            "num_attention_heads": 20,
            "hidden_size": 2560,
            "ffn_dim": 6912,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_sizes": 1261
        },
        {
            "label": "yi-6b",
            "time": 44.713429999999995,
            "batch_size": 1,
            "size": 6,
            "layers": 32,
            "num_key_value_heads": 4,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5,
            "compiled_graph_sizes": 1120
        },
        {
            "label": "yi-9b",
            "time": 60.541937600000004,
            "batch_size": 1,
            "size": 9,
            "layers": 48,
            "num_key_value_heads": 4,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 64000,
            "tokenizer_size": 3.5,
            "compiled_graph_sizes": 1693
        }
    ],
    "validation": [
        {
            "label": "falcon-11b",
            "time": 69.214082,
            "batch_size": 1,
            "size": 11,
            "layers": 60,
            "num_key_value_heads": 8,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 16384,
            "vocab_size": 65024,
            "tokenizer_size": 2.7,
            "compiled_graph_sizes": 1716
        },
        {
            "label": "gemma-7b",
            "time": 53.50291020000001,
            "batch_size": 1,
            "size": 7,
            "layers": 28,
            "num_key_value_heads": 16,
            "num_attention_heads": 16,
            "hidden_size": 3072,
            "ffn_dim": 24576,
            "vocab_size": 256000,
            "tokenizer_size": 17.5,
            "compiled_graph_sizes": 704
        },
        {
            "label": "llama3-3b",
            "time": 33.4088134,
            "batch_size": 1,
            "size": 3,
            "layers": 28,
            "num_key_value_heads": 8,
            "num_attention_heads": 24,
            "hidden_size": 3072,
            "ffn_dim": 8192,
            "vocab_size": 128256,
            "tokenizer_size": 8.7,
            "compiled_graph_sizes": 979
        },
        {
            "label": "mistral-7b",
            "time": 48.938413600000004,
            "batch_size": 1,
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 8,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 14336,
            "vocab_size": 32000,
            "tokenizer_size": 1.8,
            "compiled_graph_sizes": 1120
        },
        {
            "label": "qwen-7b",
            "time": 51.199209800000006,
            "batch_size": 1,
            "size": 7,
            "layers": 32,
            "num_key_value_heads": 32,
            "num_attention_heads": 32,
            "hidden_size": 4096,
            "ffn_dim": 11008,
            "vocab_size": 151936,
            "tokenizer_size": 6.8,
            "compiled_graph_sizes": 1002
        }
    ]
}