{
    "qwen-0.5b": {
        "size": 0.5,
        "layers": 24,
        "num_key_value_heads": 16,
        "num_attention_heads": 16,
        "hidden_size": 1024,
        "ffn_dim": 2816,
        "vocab_size": 151936,
        "tokenizer_size": 6.8,
        "compiled_graph_sizes": 695
    },
    "qwen-1.8b": {
        "size": 1.8,
        "layers": 24,
        "num_key_value_heads": 16,
        "num_attention_heads": 16,
        "hidden_size": 2048,
        "ffn_dim": 5504,
        "vocab_size": 151936,
        "tokenizer_size": 6.8,
        "compiled_graph_sizes": 747
    },
    "qwen-4b": {
        "size": 4,
        "layers": 40,
        "num_key_value_heads": 20,
        "num_attention_heads": 20,
        "hidden_size": 2560,
        "ffn_dim": 6912,
        "vocab_size": 151936,
        "tokenizer_size": 6.8,
        "compiled_graph_sizes": 1261
    },
    "qwen-7b": {
        "size": 7,
        "layers": 32,
        "num_key_value_heads": 32,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 11008,
        "vocab_size": 151936,
        "tokenizer_size": 6.8,
        "compiled_graph_sizes": 1002
    },
    "qwen-14b": {
        "size": 14,
        "layers": 40,
        "num_key_value_heads": 40,
        "num_attention_heads": 40,
        "hidden_size": 5120,
        "ffn_dim": 13696,
        "vocab_size": 152064,
        "tokenizer_size": 6.8,
        "compiled_graph_sizes": 1262
    },
    "qwen-14.3b": {
        "size": 14.3,
        "layers": 24,
        "num_key_value_heads": 16,
        "num_attention_heads": 16,
        "hidden_size": 2048,
        "ffn_dim": 1408,
        "vocab_size": 152064,
        "tokenizer_size": 6.8,
        "moe": {
            "n_experts": 60,
            "n_active_experts": 4,
            "active_size": 2.7
        },
        "compiled_graph_sizes": 808
    },
    "llama2-7b-hf": {
        "size": 7,
        "layers": 32,
        "num_key_value_heads": 32,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 11008,
        "vocab_size": 32000,
        "tokenizer_size": 1.8,
        "compiled_graph_sizes": 995
    },
    "llama2-13b-hf": {
        "size": 13,
        "layers": 40,
        "num_key_value_heads": 40,
        "num_attention_heads": 40,
        "hidden_size": 5120,
        "ffn_dim": 13824,
        "vocab_size": 32000,
        "tokenizer_size": 1.8,
        "compiled_graph_sizes": 1253
    },
    "llama3-3b": {
        "size": 3,
        "layers": 28,
        "num_key_value_heads": 8,
        "num_attention_heads": 24,
        "hidden_size": 3072,
        "ffn_dim": 8192,
        "vocab_size": 128256,
        "tokenizer_size": 8.7,
        "compiled_graph_sizes": 979
    },
    "llama3.1-8b-instruct": {
        "size": 8,
        "layers": 32,
        "num_key_value_heads": 8,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 14336,
        "vocab_size": 128256,
        "tokenizer_size": 8.7,
        "compiled_graph_sizes": 1121
    },
    "falcon-7b": {
        "size": 7,
        "layers": 32,
        "num_key_value_heads": 1,
        "num_attention_heads": 71,
        "hidden_size": 4544,
        "ffn_dim": 18176,
        "vocab_size": 65024,
        "tokenizer_size": 2.7,
        "compiled_graph_sizes": 905
    },
    "falcon-11b": {
        "size": 11,
        "layers": 60,
        "num_key_value_heads": 8,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 16384,
        "vocab_size": 65024,
        "tokenizer_size": 2.7,
        "compiled_graph_sizes": 1716
    },
    "yi-6b": {
        "size": 6,
        "layers": 32,
        "num_key_value_heads": 4,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 11008,
        "vocab_size": 64000,
        "tokenizer_size": 3.5,
        "compiled_graph_sizes": 1120
    },
    "yi-9b": {
        "size": 9,
        "layers": 48,
        "num_key_value_heads": 4,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 11008,
        "vocab_size": 64000,
        "tokenizer_size": 3.5,
        "compiled_graph_sizes": 1693
    },
    "mpt-7b": {
        "size": 7,
        "layers": 32,
        "num_key_value_heads": 32,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 16384,
        "vocab_size": 50432,
        "tokenizer_size": 2.1,
        "compiled_graph_sizes": 550
    },
    "mistral-7b": {
        "size": 7,
        "layers": 32,
        "num_key_value_heads": 8,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 14336,
        "vocab_size": 32000,
        "tokenizer_size": 1.8,
        "compiled_graph_sizes": 1120
    },
    "gemma-7b": {
        "size": 7,
        "layers": 28,
        "num_key_value_heads": 16,
        "num_attention_heads": 16,
        "hidden_size": 3072,
        "ffn_dim": 24576,
        "vocab_size": 256000,
        "tokenizer_size": 17.5,
        "compiled_graph_sizes": 704
    },
    "gpt-oss-20b": {
        "size": 21,
        "layers": 24,
        "num_key_value_heads": 8,
        "num_attention_heads": 64,
        "hidden_size": 2880,
        "ffn_dim": 2880,
        "vocab_size": 201088,
        "tokenizer_size": 27,
        "moe": {
            "n_experts": 32,
            "n_active_experts": 4,
            "active_size": 3.6
        },
        "compiled_graph_sizes": 917
    },
    "deepseek-v2-lite-16b": {
        "size": 16,
        "layers": 27,
        "num_key_value_heads": 16,
        "num_attention_heads": 16,
        "hidden_size": 2048,
        "ffn_dim": 1408,
        "vocab_size": 102400,
        "tokenizer_size": 4.4,
        "moe": {
            "n_experts": 64,
            "n_active_experts": 6,
            "active_size": 2.4
        },
        "compiled_graph_sizes": 1060
    },
    "deepseek-r1-distill-llama-8b": {
        "size": 8,
        "layers": 32,
        "num_key_value_heads": 8,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 14336,
        "vocab_size": 128256,
        "tokenizer_size": 8.7,
        "compiled_graph_sizes": 1121
    },
    "deepseek-r1-distill-qwen-7b": {
        "size": 7,
        "layers": 28,
        "num_key_value_heads": 4,
        "num_attention_heads": 28,
        "hidden_size": 3584,
        "ffn_dim": 18944,
        "vocab_size": 152064,
        "tokenizer_size": 6.8,
        "compiled_graph_sizes": 984
    },
    "granite3.3-8b-instruct": {
        "size": 8,
        "layers": 40,
        "num_key_value_heads": 8,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 12800,
        "vocab_size": 49159,
        "tokenizer_size": 3.4,
        "compiled_graph_sizes": 1374
    },
    "granite4.0-h-3b": {
        "size": 3,
        "layers": 40,
        "num_key_value_heads": 8,
        "num_attention_heads": 32,
        "hidden_size": 2048,
        "ffn_dim": 8192,
        "vocab_size": 100352,
        "tokenizer_size": 6.9,
        "compiled_graph_sizes": 810
    },
    "granite4.0-h-32b": {
        "size": 32,
        "layers": 40,
        "num_key_value_heads": 8,
        "num_attention_heads": 32,
        "hidden_size": 4096,
        "ffn_dim": 1536,
        "vocab_size": 100352,
        "tokenizer_size": 6.9,
        "moe": {
            "n_experts": 72,
            "n_active_experts": 10,
            "active_size": 9
        },
        "compiled_graph_sizes": 865
    },
    "olmoe-7b": {
        "size": 7,
        "layers": 16,
        "num_key_value_heads": 16,
        "num_attention_heads": 16,
        "hidden_size": 2048,
        "ffn_dim": 1024,
        "vocab_size": 50304,
        "tokenizer_size": 2.1,
        "moe": {
            "n_experts": 64,
            "n_active_experts": 8,
            "active_size": 1
        },
        "compiled_graph_sizes": 571
    }
}