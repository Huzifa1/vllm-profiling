{
    "qwen-0.5b": {
        "size": 0.5,
        "layers": 24,
        "attn_heads": 16,
        "attn_heads_num": 16,
        "hidden_size": 1024,
        "ffn_dim": 2816,
        "vocab_size": 151936,
        "tokenizer_size": 6.8
    },
    "qwen-1.8b": {
        "size": 1.8,
        "layers": 24,
        "attn_heads": 16,
        "attn_heads_num": 16,
        "hidden_size": 2048,
        "ffn_dim": 5504,
        "vocab_size": 151936,
        "tokenizer_size": 6.8
    },
    "qwen-4b": {
        "size": 4,
        "layers": 40,
        "attn_heads": 20,
        "attn_heads_num": 20,
        "hidden_size": 2560,
        "ffn_dim": 6912,
        "vocab_size": 151936,
        "tokenizer_size": 6.8
    },
    "qwen-7b": {
        "size": 7,
        "layers": 32,
        "attn_heads": 32,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 11008,
        "vocab_size": 151936,
        "tokenizer_size": 6.8
    },
    "qwen-14b": {
        "size": 14,
        "layers": 40,
        "attn_heads": 40,
        "attn_heads_num": 40,
        "hidden_size": 5120,
        "ffn_dim": 13696,
        "vocab_size": 152064,
        "tokenizer_size": 6.8
    },
    "qwen-14.3b": {
        "size": 14.3,
        "layers": 24,
        "attn_heads": 16,
        "attn_heads_num": 16,
        "hidden_size": 2048,
        "ffn_dim": 1408,
        "vocab_size": 152064,
        "tokenizer_size": 6.8,
        "moe": {
            "n_experts": 60,
            "n_active_experts": 4,
            "active_size": 2.7
        }
    },
    "llama2-7b-hf": {
        "size": 7,
        "layers": 32,
        "attn_heads": 32,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 11008,
        "vocab_size": 32000,
        "tokenizer_size": 1.8
    },
    "llama2-13b-hf": {
        "size": 13,
        "layers": 40,
        "attn_heads": 40,
        "attn_heads_num": 40,
        "hidden_size": 5120,
        "ffn_dim": 13824,
        "vocab_size": 32000,
        "tokenizer_size": 1.8
    },
    "llama3-3b": {
        "size": 3,
        "layers": 28,
        "attn_heads": 8,
        "attn_heads_num": 24,
        "hidden_size": 3072,
        "ffn_dim": 8192,
        "vocab_size": 128256,
        "tokenizer_size": 8.7
    },
    "falcon-7b": {
        "size": 7,
        "layers": 32,
        "attn_heads": 1,
        "attn_heads_num": 71,
        "hidden_size": 4544,
        "ffn_dim": 18176,
        "vocab_size": 65024,
        "tokenizer_size": 2.7
    },
    "falcon-11b": {
        "size": 11,
        "layers": 60,
        "attn_heads": 8,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 16384,
        "vocab_size": 65024,
        "tokenizer_size": 2.7
    },
    "yi-6b": {
        "size": 6,
        "layers": 32,
        "attn_heads": 4,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 11008,
        "vocab_size": 64000,
        "tokenizer_size": 3.5
    },
    "yi-9b": {
        "size": 9,
        "layers": 48,
        "attn_heads": 4,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 11008,
        "vocab_size": 64000,
        "tokenizer_size": 3.5
    },
    "mpt-7b": {
        "size": 7,
        "layers": 32,
        "attn_heads": 32,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 16384,
        "vocab_size": 50432,
        "tokenizer_size": 2.1
    },
    "mistral-7b": {
        "size": 7,
        "layers": 32,
        "attn_heads": 8,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 14336,
        "vocab_size": 32000,
        "tokenizer_size": 1.8
    },
    "gemma-7b": {
        "size": 7,
        "layers": 28,
        "attn_heads": 16,
        "attn_heads_num": 16,
        "hidden_size": 3072,
        "ffn_dim": 24576,
        "vocab_size": 256000,
        "tokenizer_size": 17.5
    },
    "gpt-oss-20b": {
        "size": 21,
        "layers": 24,
        "attn_heads": 8,
        "attn_heads_num": 64,
        "hidden_size": 2880,
        "ffn_dim": 2880,
        "vocab_size": 201088,
        "tokenizer_size": 27,
        "moe": {
            "n_experts": 32,
            "n_active_experts": 4,
            "active_size": 3.6
        }
    },
    "deepseek-v2-lite-16b": {
        "size": 16,
        "layers": 27,
        "attn_heads": 16,
        "attn_heads_num": 16,
        "hidden_size": 2048,
        "ffn_dim": 1408,
        "vocab_size": 102400,
        "tokenizer_size": 4.4,
        "moe": {
            "n_experts": 64,
            "n_active_experts": 6,
            "active_size": 2.4
        }
    },
    "deepseek-r1-distill-llama-8b": {
        "size": 8,
        "layers": 32,
        "attn_heads": 8,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 14336,
        "vocab_size": 128256,
        "tokenizer_size": 8.7
    },
    "deepseek-r1-distill-qwen-7b": {
        "size": 7,
        "layers": 28,
        "attn_heads": 4,
        "attn_heads_num": 28,
        "hidden_size": 3584,
        "ffn_dim": 18944,
        "vocab_size": 152064,
        "tokenizer_size": 6.8
    },
    "mixtral-46.7b": {
        "size": 46.7,
        "layers": 32,
        "attn_heads": 8,
        "attn_heads_num": 32,
        "hidden_size": 4096,
        "ffn_dim": 14336,
        "vocab_size": 32000,
        "tokenizer_size": 1.8,
        "moe": {
            "n_experts": 8,
            "n_active_experts": 2,
            "active_size": 12.9
        }
    }
}